services:
  etcd:
    image: 'bitnami/etcd:latest'
    environment:
      - ALLOW_NONE_AUTHENTICATION=yes
      - ETCD_ADVERTISE_CLIENT_URLS=http://etcd:2379
    healthcheck:
      test: ["CMD", "etcdctl", "endpoint", "health"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 5s
    networks:
      - backend_net

  llama_8b_gpu:
    build:
      context: .
      dockerfile: docker/vllm.Dockerfile
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    depends_on:
      etcd:
        condition: service_healthy
    command: >
      --model meta-llama/Llama-3.1-8B-Instruct
      --gpu-memory-utilization 0.5 
      --max-model-len 10000 
      --tensor-parallel-size 1
    environment:
      - SVC_HOST=llama_8b_gpu
      - SVC_PORT=8000
      - ETCD_HOST=etcd
      - ETCD_PORT=2379
    env_file:
      - .env
    volumes:
      - hugging_face_models:/root/.cache/huggingface  # cache models
    networks:
      - backend_net
  llama_1b_gpu:
    build:
      context: .
      dockerfile: docker/vllm.Dockerfile
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    depends_on:
      etcd:
        condition: service_healthy
    command: >
      --model meta-llama/Llama-3.2-1B-Instruct
      --gpu-memory-utilization 0.2 
      --max-model-len 10000 
      --tensor-parallel-size 1
    environment:
      - SVC_HOST=llama_1b_gpu
      - SVC_PORT=8000
      - ETCD_HOST=etcd
      - ETCD_PORT=2379
    env_file:
      - .env
    volumes:
      - hugging_face_models:/root/.cache/huggingface  # cache models
    networks:
      - backend_net
volumes:
  hugging_face_models:

networks:
  backend_net: