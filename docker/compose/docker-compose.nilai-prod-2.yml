services:
  llama_8b_gpu:
    image: nillion/nilai-vllm:latest
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

    ulimits:
      memlock: -1
      stack: 67108864
    env_file:
      - .env
    restart: unless-stopped
    depends_on:
      etcd:
        condition: service_healthy
      gpt_20b_gpu:
        # Llama takes less time to initialize
        condition: service_healthy
    command: >
      --model meta-llama/Llama-3.1-8B-Instruct
      --gpu-memory-utilization 0.23
      --max-model-len 10000
      --max-num-batched-tokens 10000
      --tensor-parallel-size 1
      --dtype bfloat16
      --kv-cache-dtype fp8
      --enable-auto-tool-choice
      --tool-call-parser llama3_json
      --uvicorn-log-level warning
      --enable-auto-tool-choice
      --chat-template /opt/vllm/templates/llama3.1_tool_json.jinja
    environment:
      - SVC_HOST=llama_8b_gpu
      - SVC_PORT=8000
      - ETCD_HOST=etcd
      - ETCD_PORT=2379
      - TOOL_SUPPORT=true
    volumes:
      - hugging_face_models:/root/.cache/huggingface
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      retries: 3
      start_period: 60s
      timeout: 10s

  gpt_20b_gpu:
    image: nillion/nilai-vllm:latest
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    ulimits:
      memlock: -1
      stack: 67108864
    env_file:
      - .env
    restart: unless-stopped
    depends_on:
      etcd:
        condition: service_healthy
    command: >
      --model openai/gpt-oss-20b
      --gpu-memory-utilization 0.75
      --max-model-len 100000
      --max-num-batched-tokens 100000
      --tensor-parallel-size 1
      --uvicorn-log-level warning
    environment:
      - SVC_HOST=gpt_20b_gpu
      - SVC_PORT=8000
      - ETCD_HOST=etcd
      - ETCD_PORT=2379
      - TOOL_SUPPORT=true
    volumes:
      - hugging_face_models:/root/.cache/huggingface  # cache models
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      retries: 3
      start_period: 180s
      timeout: 10s
volumes:
  hugging_face_models:
