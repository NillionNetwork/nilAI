services:
  llama_32_tool_gpu:
    build:
      context: .
      dockerfile: docker/vllm.Dockerfile
    deploy:
      resources:
        reservations:
          devices:
            - capabilities:
                - gpu
              driver: nvidia
    ipc: host
    depends_on:
      etcd:
        condition: service_healthy
      watt_tool_gpu:
        condition: service_healthy
    command:
      - --model
      - meta-llama/Llama-3.2-1B-Instruct
      - --max-model-len
      - "10000"
      - --device
      - cuda
      - --gpu-memory-utilization
      - "0.45"
      - --enable-auto-tool-choice
      - --tool-call-parser
      - llama3_json
      - --chat-template
      - /tmp/tool_chat_template.jinja
    env_file:
      - .env
    environment:
      SVC_HOST: "llama_32_tool_gpu"
      SVC_PORT: "8000"
      ETCD_HOST: "etcd"
      ETCD_PORT: "2379"
      TOOL_SUPPORT: true
      MODEL_ROLE: "reasoning"
    networks:
      - backend_net
    volumes:
      - type: volume
        source: hugging_face_models
        target: /root/.cache/huggingface
        volume: {}
      - type: bind
        source: $PWD/docker/compose/tool_chat_template_llama3.2_json.jinja
        target: /tmp/tool_chat_template.jinja
        bind:
          create_host_path: true
volumes:
  hugging_face_models:

networks:
  backend_net:
